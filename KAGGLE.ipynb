{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2877447",
   "metadata": {},
   "source": [
    "## Part2 Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e60c4f",
   "metadata": {},
   "source": [
    "### Pre-poccessing\n",
    "First,import some package that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b9bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b992f",
   "metadata": {},
   "source": [
    "Read the json file and then normalize it to get the 'text','id' and rename its column name. Then,read the 'emotion','identification' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b936ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"/home/neaf2070/code/ccy/dmlab2/DMlab2/DM2022-Lab2-Master-main/kaggle/tweets_DM.json\",lines=True,orient='columns')\n",
    "op=pd.json_normalize(df._source)\n",
    "op=op.rename(columns={'tweet.tweet_id':'tweet_id','tweet.text':'text'})\n",
    "ident=pd.read_csv(\"/home/neaf2070/code/ccy/dmlab2/DMlab2/DM2022-Lab2-Master-main/kaggle/data_identification.csv\")\n",
    "emo=pd.read_csv(\"/home/neaf2070/code/ccy/dmlab2/DMlab2/DM2022-Lab2-Master-main/kaggle/emotion.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfea8d6",
   "metadata": {},
   "source": [
    "And merge the files according to tweet_id and split them to get the train and test data .And drop the the hashtags column which is not important to predict emtion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf2c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total=pd.merge(op,ident,on='tweet_id')\n",
    "total=total.drop(columns='tweet.hashtags')\n",
    "train_data=total[total['identification']=='train']\n",
    "test_data=total[total['identification']=='test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad230764",
   "metadata": {},
   "source": [
    "Merge the emotion with the train data as labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44939c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.merge(train_data,emo,on='tweet_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efcbdd7",
   "metadata": {},
   "source": [
    "Drop the identification column in train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb372836",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.drop(columns='identification')\n",
    "test_data=test_data.drop(columns='identification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4c4b13",
   "metadata": {},
   "source": [
    "Finally,save the train and test data which are pre-processing .Pickle file can save the read time and space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2704a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_pickle(\"train_df.pkl\") \n",
    "test_data.to_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ca54a",
   "metadata": {},
   "source": [
    "Read the train and test pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb177063",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_pickle(\"D:\\gitresp\\dmlab2\\DM2022-Lab2-Master-main\\DM2022-Lab2-Master-main\\\\new\\\\train_df.pkl\")\n",
    "test_data=pd.read_pickle(\"D:\\gitresp\\dmlab2\\DM2022-Lab2-Master-main\\DM2022-Lab2-Master-main\\\\new\\\\test_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe14930",
   "metadata": {},
   "source": [
    "Print them to oberserve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "031f5c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tweet_id                                               text       emotion\n",
      "0  0x376b20  People who post \"add me on #Snapchat\" must be ...  anticipation\n",
      "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...       sadness\n",
      "2  0x1cd5b0                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>          fear\n",
      "3  0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...           joy\n",
      "4  0x2c91a8       Still waiting on those supplies Liscus. <LH>  anticipation\n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d355523",
   "metadata": {},
   "source": [
    "I found that '#' and \"@' are not important to predict so i replace both of them ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "010ee179",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text']=train_data['text'].replace('#',\"\")\n",
    "train_data['text']=train_data['text'].replace('@',\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3e60c",
   "metadata": {},
   "source": [
    "Using bag of words to get the max_features appears with nltk.word_tokenize and fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0af41b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ccy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(max_features=10000,\n",
       "                tokenizer=&lt;function word_tokenize at 0x00000242B8495040&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_features=10000,\n",
       "                tokenizer=&lt;function word_tokenize at 0x00000242B8495040&gt;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(max_features=10000,\n",
       "                tokenizer=<function word_tokenize at 0x00000242B8495040>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW = CountVectorizer(max_features=10000, tokenizer=nltk.word_tokenize)\n",
    "BOW.fit(train_data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe326390",
   "metadata": {},
   "source": [
    "Split train data with train and valid part.(85:15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "961150a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_da, val_da = train_test_split(train_data, random_state=777, train_size=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265200e",
   "metadata": {},
   "source": [
    "Check the valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98fa3243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320379</th>\n",
       "      <td>0x2968c1</td>\n",
       "      <td>Love isnâ€™t all we needâ€”love is all there is. &lt;...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526228</th>\n",
       "      <td>0x38798b</td>\n",
       "      <td>Happy @DoctorWho_BBCA day ðŸ’™ #WhovianLove &lt;LH&gt; ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916717</th>\n",
       "      <td>0x2320b6</td>\n",
       "      <td>I AM THE LUCKIEST PERSON! NO! JOKE! I AM REALL...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728692</th>\n",
       "      <td>0x30e9da</td>\n",
       "      <td>Whooooa is Lord Sugar even Jewish?! This is 11...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116015</th>\n",
       "      <td>0x32e844</td>\n",
       "      <td>@cbs your app has issues, love the show &lt;LH&gt;  ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941900</th>\n",
       "      <td>0x2ce6d9</td>\n",
       "      <td>@PositiveHitsPER 10K Reasons &lt;LH&gt;</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251283</th>\n",
       "      <td>0x264ca3</td>\n",
       "      <td>@FBC_Shelby_NC @CleveCoSchools Wow! To God be ...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444155</th>\n",
       "      <td>0x34b044</td>\n",
       "      <td>&lt;LH&gt; bought crackers asked market b4 returning...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260967</th>\n",
       "      <td>0x3125dc</td>\n",
       "      <td>@espn Everybody Sports Programing New York. #y...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68124</th>\n",
       "      <td>0x230766</td>\n",
       "      <td>@drakkarklose outstanding performance last nig...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>218335 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  emotion\n",
       "320379   0x2968c1  Love isnâ€™t all we needâ€”love is all there is. <...      joy\n",
       "526228   0x38798b  Happy @DoctorWho_BBCA day ðŸ’™ #WhovianLove <LH> ...      joy\n",
       "916717   0x2320b6  I AM THE LUCKIEST PERSON! NO! JOKE! I AM REALL...      joy\n",
       "728692   0x30e9da  Whooooa is Lord Sugar even Jewish?! This is 11...  sadness\n",
       "1116015  0x32e844  @cbs your app has issues, love the show <LH>  ...    anger\n",
       "...           ...                                                ...      ...\n",
       "941900   0x2ce6d9                  @PositiveHitsPER 10K Reasons <LH>      joy\n",
       "1251283  0x264ca3  @FBC_Shelby_NC @CleveCoSchools Wow! To God be ...    trust\n",
       "1444155  0x34b044  <LH> bought crackers asked market b4 returning...  disgust\n",
       "1260967  0x3125dc  @espn Everybody Sports Programing New York. #y...  disgust\n",
       "68124    0x230766  @drakkarklose outstanding performance last nig...    trust\n",
       "\n",
       "[218335 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b08bbd",
   "metadata": {},
   "source": [
    "Change the variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3883fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x=train_da\n",
    "val_x=val_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2f74ea",
   "metadata": {},
   "source": [
    "Transform them to train and valid data.And set the train_y which is called labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f05f4313",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=BOW.transform(tr_x['text'])\n",
    "val_x=BOW.transform(val_x['text'])\n",
    "train_y=tr_x['emotion']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3052b096",
   "metadata": {},
   "source": [
    "Also set the variable with labels of valid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97199dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_y=val_da['emotion']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10131414",
   "metadata": {},
   "source": [
    "Transform test data with bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30cb6e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x=BOW.transform(test_data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee9919",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1eff5e",
   "metadata": {},
   "source": [
    "#### 1. Logistic regression model\n",
    "First, i test the Logisticregression model and the valid accuracy up to 0.5427."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21838654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "c:\\Users\\ccy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.7min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lg_model=LogisticRegression(max_iter=200,verbose=True,C=0.5)\n",
    "lg_model=lg_model.fit(train_x,train_y)\n",
    "val_pre_lg=lg_model.predict(val_x)\n",
    "train_pre_lg=lg_model.predict(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77112b42",
   "metadata": {},
   "source": [
    "And the train and test accuracy are similar .Maybe it is not train well so i raise the epochs but the valid accuracy is lower the the origin. Finally i choose the origin set to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab19d429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accu: 0.5487783981610503\n",
      "val accu: 0.5427302081663499\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"train accu:\",accuracy_score(train_pre_lg,train_y))\n",
    "print(\"val accu:\",accuracy_score(val_pre_lg,val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76abd9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pre_lg=lg_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe6d663",
   "metadata": {},
   "source": [
    "Save the test pedict result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "855566e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output= pd.DataFrame({'id':test_data.tweet_id , 'emotion': test_pre_lg})\n",
    "output.to_csv('lg_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1186c72",
   "metadata": {},
   "source": [
    "Change the labels types with binary using encoder ,decoder.Use this way can let the string labels to binary value and it can train on the linear model and MLP regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ada6bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_y)\n",
    "print( label_encoder.classes_)\n",
    "\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faca2716",
   "metadata": {},
   "source": [
    "#### 2.Linear model\n",
    "I test the linear model find that the valid accuracy is 0.539"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c81e9a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "li_model=linear_model.LinearRegression()\n",
    "li_model=li_model.fit(train_x,y_train)\n",
    "li_pre=li_model.predict(train_x)\n",
    "li_pre = label_decode(label_encoder, li_pre)\n",
    "li_pre_val=li_model.predict(val_x)\n",
    "li_pre_val = label_decode(label_encoder, li_pre_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31edef23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accu: 0.5469323358346239\n",
      "val accu: 0.5395012251814871\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"train accu:\",accuracy_score(li_pre,train_y))\n",
    "print(\"val accu:\",accuracy_score(li_pre_val,val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f855c",
   "metadata": {},
   "source": [
    "Train the test data which are our kaggle compete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d05aa373",
   "metadata": {},
   "outputs": [],
   "source": [
    "li_pre_te=li_model.predict(test_x)\n",
    "li_pre_te= label_decode(label_encoder, li_pre_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5575b",
   "metadata": {},
   "source": [
    "Save the predict result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf94fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "output3= pd.DataFrame({'id':test_data.tweet_id , 'emotion': li_pre_te})\n",
    "output3.to_csv('li_pre.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123bdec1",
   "metadata": {},
   "source": [
    "#### 3.MLP regressor model\n",
    "\n",
    "I try the mlp regressor model and set the 3 hidden layers each with 32 activations.We can find that the valid accuracy up to 0.573. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "758732ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.04124775\n",
      "Iteration 2, loss = 0.03878980\n",
      "Iteration 3, loss = 0.03847716\n",
      "Iteration 4, loss = 0.03822937\n",
      "Iteration 5, loss = 0.03796708\n",
      "Iteration 6, loss = 0.03772373\n",
      "Iteration 7, loss = 0.03755805\n",
      "Iteration 8, loss = 0.03745556\n",
      "Iteration 9, loss = 0.03740067\n",
      "Iteration 10, loss = 0.03737777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ccy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "NN=MLPRegressor(activation='logistic',solver='adam',alpha=0.00007,max_iter=10,hidden_layer_sizes=(32,32,32),verbose=True,early_stopping=False)\n",
    "NN=NN.fit(train_x,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36264366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accu: 0.602415237935126\n",
      "val accu: 0.5731101289303135\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "mlp_pre=NN.predict(train_x)\n",
    "mlp_pre_val=NN.predict(val_x)\n",
    "mlp_pre=label_decode(label_encoder,mlp_pre)\n",
    "mlp_pre_val=label_decode(label_encoder,mlp_pre_val)\n",
    "print(\"train accu:\",accuracy_score(mlp_pre,train_y))\n",
    "print(\"val accu:\",accuracy_score(mlp_pre_val,val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85da2aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pre_test=NN.predict(test_x)\n",
    "mlp_pre_test=label_decode(label_encoder,mlp_pre_test)\n",
    "output5= pd.DataFrame({'id':test_data.tweet_id , 'emotion': mlp_pre_test})\n",
    "output5.to_csv('mlp_pre_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339da123",
   "metadata": {},
   "source": [
    "#### 4.MLP classifier model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a95e8",
   "metadata": {},
   "source": [
    "Using the three hidden layers and each with 32 activation we can find the valid accuracy up to 0.566."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "651357bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.28249268\n",
      "Iteration 2, loss = 1.20034489\n",
      "Iteration 3, loss = 1.17032966\n",
      "Iteration 4, loss = 1.15259740\n",
      "Iteration 5, loss = 1.14072918\n",
      "Iteration 6, loss = 1.13118169\n",
      "Iteration 7, loss = 1.12398284\n",
      "Iteration 8, loss = 1.11776398\n",
      "Iteration 9, loss = 1.11273163\n",
      "Iteration 10, loss = 1.10823756\n",
      "Iteration 11, loss = 1.10443726\n",
      "Iteration 12, loss = 1.10076523\n",
      "Iteration 13, loss = 1.09773197\n",
      "Iteration 14, loss = 1.09468995\n",
      "Iteration 15, loss = 1.09202421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ccy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "NN2=MLPClassifier(activation='tanh',solver='adam',alpha=0.001,power_t=0.5,max_iter=15,hidden_layer_sizes=(32,32,32),verbose=True,early_stopping=False)\n",
    "NN2=NN2.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "009b2a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accu: 0.636182659946267\n",
      "val accu: 0.5662353722490668\n"
     ]
    }
   ],
   "source": [
    "mlp_pre2=NN2.predict(train_x)\n",
    "mlp_pre_val2=NN2.predict(val_x)\n",
    "print(\"train accu:\",accuracy_score(mlp_pre2,train_y))\n",
    "print(\"val accu:\",accuracy_score(mlp_pre_val2,val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad7ee71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pre_test2=NN2.predict(test_x)\n",
    "output5= pd.DataFrame({'id':test_data.tweet_id , 'emotion': mlp_pre_test2})\n",
    "output5.to_csv('mlp_pre_cla.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f086a",
   "metadata": {},
   "source": [
    "Finally,i choose the MLP regressor to be my predict model because it has the highest accuracy than other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b6a5ac",
   "metadata": {},
   "source": [
    "### The things i find\n",
    "If choose the more bag of words to input to model .The accuracy will raise.The activation in hidden layer should be identity,the efficient is higher the use other different activations.The more activations MLP model has the higher accuracy the model can predict.But the time will raise simutaneously.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1d06f24a818e365ebcc6834d17d7f4108c28cf20618796cdaea72acf53c5520"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
